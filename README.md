# LLM Security Resources

This repository is a comprehensive collection of resources, links, and tutorials on Large Language Model (LLM) security. It covers various attacks and how to perform them, as well as defenses and best practices.

## Table of Contents

1. [Attacks](attacks/)
[LLMSecurity] (https://llmsecurity.net/)
Adversarial
A LLM Assisted Exploitation of AI-Guardian
Adversarial Attacks on Tables with Entity Swap
Adversarial Demonstration Attacks on Large Language Models
Adversarial Examples Are Not Bugs, They Are Features üå∂Ô∏è
Are Aligned Language Models ‚ÄúAdversarially Aligned‚Äù? üå∂Ô∏è
Bad Characters: Imperceptible NLP Attacks
Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack
Expanding Scope: Adapting English Adversarial Attacks to Chinese
Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
Gradient-based Adversarial Attacks against Text Transformers
Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models
Sample Attackability in Natural Language Adversarial Attacks
Universal and Transferable Adversarial Attacks on Aligned Language Models
Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP üå∂Ô∏è
2. [Defenses](defenses/)
3. [Tutorials](tutorials/)
4. [References](references/)

## Contributing

Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) before submitting a pull request.
