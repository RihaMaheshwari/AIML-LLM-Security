## Understanding
[Understanding AI Attacks](https://iterasec.com/blog/understanding-ai-attacks-and-their-types/)
[RAG](https://www.promptingguide.ai/research/rag)
[Research](https://www.promptingguide.ai/research)
[Security Attacks](https://maheshwari-bittu.medium.com/security-attacks-in-ai-ml-a67c775ec5e2)

## Input Manipulation Attack
[OWASP](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML01_2023-Input_Manipulation_Attack)
[AISE](https://my.ai.se/resources/3229)
[Geeks](https://www.researchgate.net/publication/349037772_Manipulation_Attacks_in_Local_Differential_Privacy)

## Adversarial Attack
[TADA](https://ceur-ws.org/Vol-3462/TADA4.pdf)

## Prompt Injection
[IBM](https://www.ibm.com/think/topics/prompt-injection)
[Learn Prompting](https://learnprompting.org/docs/prompt_hacking/injection)
[Prompting Guide](https://www.promptingguide.ai/risks/adversarial)
[Less Wrong](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post)
[Simon Willison](https://simonwillison.net/2022/Sep/12/prompt-injection/)
[Cobalt](https://www.cobalt.io/blog/prompt-injection-attacks)
[BugCrowd](https://www.bugcrowd.com/blog/ai-vulnerability-deep-dive-prompt-injection/)
[Unite AI](https://www.unite.ai/prompt-hacking-and-misuse-of-llm/?trk=article-ssr-frontend-pulse_little-text-block)
[Simon Willison](https://simonwillison.net/2023/May/2/prompt-injection-explained/)
[Vicki Li](https://vickieli.medium.com/hacking-llms-with-prompt-injections-6a5ebffb182b)

## LLM Attacks
[Portswigger](https://portswigger.net/web-security/llm-attacks)
[LLM Security](https://llmsecurity.net/)

## Threat Modelling
[AI Village](https://aivillage.org/large%20language%20models/threat-modeling-llm/)

## Jail Breaking
[Alignment Forum](https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking)

## Playground
[Playground](https://platform.openai.com/playground)

## Security Games
[Gandalf](https://gandalf.lakera.ai/baseline)